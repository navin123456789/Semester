{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ehq7p0uiTgCY",
        "outputId": "7419ccb9-5104-40e8-80bd-4a36673dee44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcbUWqgTL4bs"
      },
      "source": [
        "Stacked_RBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2C-_CW-qrnuR",
        "outputId": "39a5d96c-5a0c-4553-98c7-dce2bf2d28cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building and training CDBN...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 1.1216\n",
            "Epoch 2/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.9837\n",
            "Epoch 3/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.9601\n",
            "Epoch 4/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.9220\n",
            "Epoch 5/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.8646\n",
            "Epoch 6/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.7894\n",
            "Epoch 7/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.7314\n",
            "Epoch 8/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.7036\n",
            "Epoch 9/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.6916\n",
            "Epoch 10/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.6836\n",
            "Adding classifier and fine-tuning...\n",
            "Epoch 1/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 146ms/step - accuracy: 0.6629 - loss: 0.0000e+00 - val_accuracy: 0.9438 - val_loss: 0.0000e+00\n",
            "Epoch 2/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.9482 - loss: 0.0000e+00 - val_accuracy: 0.9500 - val_loss: 0.0000e+00\n",
            "Epoch 3/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.9605 - loss: 0.0000e+00 - val_accuracy: 0.9812 - val_loss: 0.0000e+00\n",
            "Epoch 4/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.9660 - loss: 0.0000e+00 - val_accuracy: 0.9812 - val_loss: 0.0000e+00\n",
            "Epoch 5/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - accuracy: 0.9828 - loss: 0.0000e+00 - val_accuracy: 0.9812 - val_loss: 0.0000e+00\n",
            "Epoch 6/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - accuracy: 0.9792 - loss: 0.0000e+00 - val_accuracy: 0.9625 - val_loss: 0.0000e+00\n",
            "Epoch 7/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - accuracy: 0.9653 - loss: 0.0000e+00 - val_accuracy: 0.9750 - val_loss: 0.0000e+00\n",
            "Epoch 8/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.9925 - loss: 0.0000e+00 - val_accuracy: 0.9750 - val_loss: 0.0000e+00\n",
            "Epoch 9/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.9838 - loss: 0.0000e+00 - val_accuracy: 0.9750 - val_loss: 0.0000e+00\n",
            "Epoch 10/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.9887 - loss: 0.0000e+00 - val_accuracy: 0.9812 - val_loss: 0.0000e+00\n",
            "Epoch 11/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - accuracy: 0.9854 - loss: 0.0000e+00 - val_accuracy: 0.9875 - val_loss: 0.0000e+00\n",
            "Epoch 12/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - accuracy: 0.9901 - loss: 0.0000e+00 - val_accuracy: 0.9750 - val_loss: 0.0000e+00\n",
            "Epoch 13/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - accuracy: 0.9808 - loss: 0.0000e+00 - val_accuracy: 0.9750 - val_loss: 0.0000e+00\n",
            "Epoch 14/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9836 - loss: 0.0000e+00 - val_accuracy: 0.9750 - val_loss: 0.0000e+00\n",
            "Epoch 15/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.9893 - loss: 0.0000e+00 - val_accuracy: 0.9875 - val_loss: 0.0000e+00\n",
            "Epoch 16/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.9962 - loss: 0.0000e+00 - val_accuracy: 0.9875 - val_loss: 0.0000e+00\n",
            "Epoch 17/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.9909 - loss: 0.0000e+00 - val_accuracy: 0.9875 - val_loss: 0.0000e+00\n",
            "Epoch 18/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - accuracy: 0.9782 - loss: 0.0000e+00 - val_accuracy: 0.9875 - val_loss: 0.0000e+00\n",
            "Epoch 19/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - accuracy: 0.9778 - loss: 0.0000e+00 - val_accuracy: 0.9875 - val_loss: 0.0000e+00\n",
            "Epoch 20/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.9816 - loss: 0.0000e+00 - val_accuracy: 0.9875 - val_loss: 0.0000e+00\n",
            "Evaluating on test set...\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 468ms/step\n",
            "Test Accuracy: 98.00%\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load and preprocess data\n",
        "def load_data(dataset_path, max_files=1000):\n",
        "    spectrograms = []\n",
        "    labels = []\n",
        "    for file_name in os.listdir(dataset_path)[:max_files]:\n",
        "        if file_name.endswith(\".mp3\"):\n",
        "            try:\n",
        "                # Extract genre from file name (format: genre_index.mp3)\n",
        "                genre = file_name.split(\"_\")[0]\n",
        "                file_path = os.path.join(dataset_path, file_name)\n",
        "\n",
        "                # Load audio file (5 seconds) and extract Mel-spectrogram\n",
        "                audio, sr = librosa.load(file_path, sr=22050, duration=5)  # Load 5 seconds\n",
        "                spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128, fmax=8000)\n",
        "                spectrogram = librosa.power_to_db(spectrogram, ref=np.max)  # Convert to dB scale\n",
        "                spectrogram = np.expand_dims(spectrogram, axis=-1)  # Add channel dimension\n",
        "                spectrograms.append(spectrogram)\n",
        "                labels.append(genre)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {file_path}: {e}\")\n",
        "    return np.array(spectrograms), np.array(labels)\n",
        "\n",
        "# Normalize spectrograms\n",
        "def normalize_spectrograms(spectrograms):\n",
        "    mean = np.mean(spectrograms, axis=(1, 2), keepdims=True)\n",
        "    std = np.std(spectrograms, axis=(1, 2), keepdims=True)\n",
        "    return (spectrograms - mean) / (std + 1e-8)\n",
        "\n",
        "# Step 2: Define CDBN (Convolutional Deep Belief Network)\n",
        "def build_cdbn(input_shape):\n",
        "    model = models.Sequential([\n",
        "        # First Convolutional RBM\n",
        "        layers.Conv2D(32, (3, 3), activation='sigmoid', padding='same', input_shape=input_shape),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        # Second Convolutional RBM\n",
        "        layers.Conv2D(64, (3, 3), activation='sigmoid', padding='same'),\n",
        "        layers.UpSampling2D(size=(2, 2))  # Upsample to match input dimensions\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Step 3: Train CDBN (unsupervised pre-training)\n",
        "def train_cdbn(cdbn, spectrograms):\n",
        "    cdbn.compile(optimizer='adam', loss='mse')\n",
        "    cdbn.fit(spectrograms, spectrograms, epochs=10, batch_size=32, shuffle=True)\n",
        "\n",
        "# Step 4: Add classifier and fine-tune\n",
        "def add_classifier(cdbn, num_classes):\n",
        "    model = models.Sequential([\n",
        "        cdbn,\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred = np.argmax(y_pred, axis=1)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Main execution\n",
        "# Load dataset\n",
        "dataset_path = \"/content/drive/MyDrive/New_indexes_mp3\"  # Replace with your dataset path\n",
        "spectrograms, labels = load_data(dataset_path, max_files=1000)  # Load up to 1000 files\n",
        "spectrograms = normalize_spectrograms(spectrograms)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "labels_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(spectrograms, labels_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build and train CDBN (unsupervised pre-training)\n",
        "print(\"Building and training CDBN...\")\n",
        "input_shape = X_train.shape[1:]  # Get input shape from data\n",
        "cdbn = build_cdbn(input_shape)\n",
        "train_cdbn(cdbn, X_train)\n",
        "\n",
        "# Add classifier and fine-tune\n",
        "print(\"Adding classifier and fine-tuning...\")\n",
        "num_classes = len(label_encoder.classes_)\n",
        "model = add_classifier(cdbn, num_classes)\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate on test set\n",
        "print(\"Evaluating on test set...\")\n",
        "evaluate_model(model, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hia9BBdKvJ7T",
        "outputId": "43aaa1ed-454e-492a-f849-b1bc5e3044aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "Predicted Genre: Gazal\n"
          ]
        }
      ],
      "source": [
        "def predict_genre(model, file_path, label_encoder):\n",
        "\n",
        "    # Load and preprocess the audio file\n",
        "    audio, sr = librosa.load(file_path, sr=22050, duration=5)  # Load 5 seconds\n",
        "    spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128, fmax=8000)\n",
        "    spectrogram = librosa.power_to_db(spectrogram, ref=np.max)  # Convert to dB scale\n",
        "    spectrogram = np.expand_dims(spectrogram, axis=-1)  # Add channel dimension\n",
        "    spectrogram = normalize_spectrograms(np.array([spectrogram]))  # Normalize\n",
        "\n",
        "    # Predict genre\n",
        "    prediction = model.predict(spectrogram)\n",
        "    predicted_class = np.argmax(prediction, axis=1)\n",
        "    predicted_genre = label_encoder.inverse_transform(predicted_class)\n",
        "    return predicted_genre[0]\n",
        "\n",
        " # Predict genre for a new audio file\n",
        "new_audio_path = \"/content/drive/MyDrive/New_indexes_mp3/Gazal_389.mp3\"  # Replace with the path to your new audio file\n",
        "predicted_genre = predict_genre(model, new_audio_path, label_encoder)\n",
        "print(f\"Predicted Genre: {predicted_genre}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCocPyaRtlMd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxuJicfZznvy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cr0KX7x_UnDn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
