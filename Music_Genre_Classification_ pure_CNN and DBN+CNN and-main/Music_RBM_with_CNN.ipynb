{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nujdEHIQM2by",
        "outputId": "87832a77-8ab3-4b64-e3e6-df4651c2d124"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naya"
      ],
      "metadata": {
        "id": "0Y76EW9dvxLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "o37mfdSfwDdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CRBM Class Definition (for unsupervised pretraining)\n",
        "class CRBM:\n",
        "    def __init__(self, n_visible, n_hidden, learning_rate=0.01):\n",
        "        self.n_visible = n_visible\n",
        "        self.n_hidden = n_hidden\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weights = np.random.normal(0, 0.1, (n_visible, n_hidden))\n",
        "        self.h_bias = np.zeros(n_hidden)\n",
        "        self.v_bias = np.zeros(n_visible)\n",
        "\n",
        "    def sample_h(self, v):\n",
        "        h_prob = self._sigmoid(np.dot(v, self.weights) + self.h_bias)\n",
        "        return h_prob, np.random.binomial(1, h_prob)\n",
        "\n",
        "    def sample_v(self, h):\n",
        "        v_prob = self._sigmoid(np.dot(h, self.weights.T) + self.v_bias)\n",
        "        return v_prob, np.random.binomial(1, v_prob)\n",
        "\n",
        "    def train(self, data, epochs=100):\n",
        "        for epoch in range(epochs):\n",
        "            v0 = data\n",
        "            h0_prob, h0 = self.sample_h(v0)\n",
        "            v1_prob, v1 = self.sample_v(h0)\n",
        "            h1_prob, h1 = self.sample_h(v1)\n",
        "            self.weights += np.dot(v0.T, h0 - h1) * self.learning_rate\n",
        "            self.v_bias += np.mean(v0 - v1, axis=0) * self.learning_rate\n",
        "            self.h_bias += np.mean(h0 - h1, axis=0) * self.learning_rate\n",
        "\n",
        "    def _sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Data Preparation: Load the audio files and extract features\n",
        "data_dir = '/content/drive/MyDrive/New_indexes_mp3'  # Path to the directory containing genre_index.mp3 files\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "# Loop through all files in the directory\n",
        "for filename in os.listdir(data_dir):\n",
        "    if filename.endswith('.mp3'):\n",
        "        # Extract genre from the filename\n",
        "        genre = filename.split('_')[0]  # Assuming format is 'genre_index.mp3'\n",
        "        file_path = os.path.join(data_dir, filename)\n",
        "\n",
        "        # Load audio and extract Mel spectrogram\n",
        "        signal, sr = librosa.load(file_path, sr=22050)\n",
        "        mel_spec = librosa.feature.melspectrogram(y=signal, sr=sr)\n",
        "        mel_spec_db = librosa.power_to_db(mel_spec)  # Convert to decibel scale for better representation\n",
        "        feature = mel_spec_db.flatten()  # Flatten the Mel spectrogram into a 1D vector\n",
        "\n",
        "        # Append the features and labels\n",
        "        features.append(feature)\n",
        "        labels.append(genre)\n",
        "\n",
        "# Convert features to numpy array\n",
        "X = np.array(features)\n",
        "\n",
        "# Label Encoding\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(labels)  # Convert labels to integers\n",
        "y_onehot = to_categorical(y_encoded)  # One-hot encode labels\n",
        "\n",
        "# Normalize features\n",
        "X = (X - np.mean(X)) / np.std(X)  # Standard normalization\n",
        "\n",
        "# Padding features to make all vectors the same length\n",
        "max_length = max([f.shape[0] for f in features])  # Find the max length of the features\n",
        "X = pad_sequences(X, maxlen=max_length, padding='post', truncating='post')  # Pad sequences to max length\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "JUpqhr1RwNB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CRBM Pretraining\n",
        "num_layers = 2\n",
        "hidden_units = 64\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "rbm_input = X_train\n",
        "rbm_layers = []\n",
        "\n",
        "# Train the CRBM layers\n",
        "for layer in range(num_layers):\n",
        "    rbm = CRBM(n_visible=rbm_input.shape[1], n_hidden=hidden_units)\n",
        "    print(f\"\\nInitializing Layer {layer + 1} with {hidden_units} hidden units...\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "        for i in range(0, rbm_input.shape[0], batch_size):\n",
        "            batch = rbm_input[i:i + batch_size]\n",
        "            rbm.train(batch, epochs=1)  # Train for 1 epoch per batch\n",
        "            print(f\"    Processed batch {i//batch_size + 1}/{rbm_input.shape[0]//batch_size + 1}\")\n",
        "\n",
        "    rbm_layers.append(rbm)\n",
        "    print(f\"Layer {layer + 1} training complete.\")\n",
        "    _, rbm_input = rbm.sample_h(rbm_input)  # Transform data to hidden representation for next layer\n",
        "    print(f\"Transformed data to hidden representation for next layer.\\n\")\n",
        "\n",
        "print(\"RBM Pretraining Complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Yi1fpbawPLm",
        "outputId": "b6fa11e1-54e9-420d-811f-f9d0eb5147b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Initializing Layer 1 with 64 hidden units...\n",
            "Epoch 1/10\n",
            "    Processed batch 1/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-cd4541fada9f>:30: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-x))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Processed batch 2/40\n",
            "    Processed batch 3/40\n",
            "    Processed batch 4/40\n",
            "    Processed batch 5/40\n",
            "    Processed batch 6/40\n",
            "    Processed batch 7/40\n",
            "    Processed batch 8/40\n",
            "    Processed batch 9/40\n",
            "    Processed batch 10/40\n",
            "    Processed batch 11/40\n",
            "    Processed batch 12/40\n",
            "    Processed batch 13/40\n",
            "    Processed batch 14/40\n",
            "    Processed batch 15/40\n",
            "    Processed batch 16/40\n",
            "    Processed batch 17/40\n",
            "    Processed batch 18/40\n",
            "    Processed batch 19/40\n",
            "    Processed batch 20/40\n",
            "    Processed batch 21/40\n",
            "    Processed batch 22/40\n",
            "    Processed batch 23/40\n",
            "    Processed batch 24/40\n",
            "    Processed batch 25/40\n",
            "    Processed batch 26/40\n",
            "    Processed batch 27/40\n",
            "    Processed batch 28/40\n",
            "    Processed batch 29/40\n",
            "    Processed batch 30/40\n",
            "    Processed batch 31/40\n",
            "    Processed batch 32/40\n",
            "    Processed batch 33/40\n",
            "    Processed batch 34/40\n",
            "    Processed batch 35/40\n",
            "    Processed batch 36/40\n",
            "    Processed batch 37/40\n",
            "    Processed batch 38/40\n",
            "    Processed batch 39/40\n",
            "    Processed batch 40/40\n",
            "Epoch 2/10\n",
            "    Processed batch 1/40\n",
            "    Processed batch 2/40\n",
            "    Processed batch 3/40\n",
            "    Processed batch 4/40\n",
            "    Processed batch 5/40\n",
            "    Processed batch 6/40\n",
            "    Processed batch 7/40\n",
            "    Processed batch 8/40\n",
            "    Processed batch 9/40\n",
            "    Processed batch 10/40\n",
            "    Processed batch 11/40\n",
            "    Processed batch 12/40\n",
            "    Processed batch 13/40\n",
            "    Processed batch 14/40\n",
            "    Processed batch 15/40\n",
            "    Processed batch 16/40\n",
            "    Processed batch 17/40\n",
            "    Processed batch 18/40\n",
            "    Processed batch 19/40\n",
            "    Processed batch 20/40\n",
            "    Processed batch 21/40\n",
            "    Processed batch 22/40\n",
            "    Processed batch 23/40\n",
            "    Processed batch 24/40\n",
            "    Processed batch 25/40\n",
            "    Processed batch 26/40\n",
            "    Processed batch 27/40\n",
            "    Processed batch 28/40\n",
            "    Processed batch 29/40\n",
            "    Processed batch 30/40\n",
            "    Processed batch 31/40\n",
            "    Processed batch 32/40\n",
            "    Processed batch 33/40\n",
            "    Processed batch 34/40\n",
            "    Processed batch 35/40\n",
            "    Processed batch 36/40\n",
            "    Processed batch 37/40\n",
            "    Processed batch 38/40\n",
            "    Processed batch 39/40\n",
            "    Processed batch 40/40\n",
            "Epoch 3/10\n",
            "    Processed batch 1/40\n",
            "    Processed batch 2/40\n",
            "    Processed batch 3/40\n",
            "    Processed batch 4/40\n",
            "    Processed batch 5/40\n",
            "    Processed batch 6/40\n",
            "    Processed batch 7/40\n",
            "    Processed batch 8/40\n",
            "    Processed batch 9/40\n",
            "    Processed batch 10/40\n",
            "    Processed batch 11/40\n",
            "    Processed batch 12/40\n",
            "    Processed batch 13/40\n",
            "    Processed batch 14/40\n",
            "    Processed batch 15/40\n",
            "    Processed batch 16/40\n",
            "    Processed batch 17/40\n",
            "    Processed batch 18/40\n",
            "    Processed batch 19/40\n",
            "    Processed batch 20/40\n",
            "    Processed batch 21/40\n",
            "    Processed batch 22/40\n",
            "    Processed batch 23/40\n",
            "    Processed batch 24/40\n",
            "    Processed batch 25/40\n",
            "    Processed batch 26/40\n",
            "    Processed batch 27/40\n",
            "    Processed batch 28/40\n",
            "    Processed batch 29/40\n",
            "    Processed batch 30/40\n",
            "    Processed batch 31/40\n",
            "    Processed batch 32/40\n",
            "    Processed batch 33/40\n",
            "    Processed batch 34/40\n",
            "    Processed batch 35/40\n",
            "    Processed batch 36/40\n",
            "    Processed batch 37/40\n",
            "    Processed batch 38/40\n",
            "    Processed batch 39/40\n",
            "    Processed batch 40/40\n",
            "Epoch 4/10\n",
            "    Processed batch 1/40\n",
            "    Processed batch 2/40\n",
            "    Processed batch 3/40\n",
            "    Processed batch 4/40\n",
            "    Processed batch 5/40\n",
            "    Processed batch 6/40\n",
            "    Processed batch 7/40\n",
            "    Processed batch 8/40\n",
            "    Processed batch 9/40\n",
            "    Processed batch 10/40\n",
            "    Processed batch 11/40\n",
            "    Processed batch 12/40\n",
            "    Processed batch 13/40\n",
            "    Processed batch 14/40\n",
            "    Processed batch 15/40\n",
            "    Processed batch 16/40\n",
            "    Processed batch 17/40\n",
            "    Processed batch 18/40\n",
            "    Processed batch 19/40\n",
            "    Processed batch 20/40\n",
            "    Processed batch 21/40\n",
            "    Processed batch 22/40\n",
            "    Processed batch 23/40\n",
            "    Processed batch 24/40\n",
            "    Processed batch 25/40\n",
            "    Processed batch 26/40\n",
            "    Processed batch 27/40\n",
            "    Processed batch 28/40\n",
            "    Processed batch 29/40\n",
            "    Processed batch 30/40\n",
            "    Processed batch 31/40\n",
            "    Processed batch 32/40\n",
            "    Processed batch 33/40\n",
            "    Processed batch 34/40\n",
            "    Processed batch 35/40\n",
            "    Processed batch 36/40\n",
            "    Processed batch 37/40\n",
            "    Processed batch 38/40\n",
            "    Processed batch 39/40\n",
            "    Processed batch 40/40\n",
            "Epoch 5/10\n",
            "    Processed batch 1/40\n",
            "    Processed batch 2/40\n",
            "    Processed batch 3/40\n",
            "    Processed batch 4/40\n",
            "    Processed batch 5/40\n",
            "    Processed batch 6/40\n",
            "    Processed batch 7/40\n",
            "    Processed batch 8/40\n",
            "    Processed batch 9/40\n",
            "    Processed batch 10/40\n",
            "    Processed batch 11/40\n",
            "    Processed batch 12/40\n",
            "    Processed batch 13/40\n",
            "    Processed batch 14/40\n",
            "    Processed batch 15/40\n",
            "    Processed batch 16/40\n",
            "    Processed batch 17/40\n",
            "    Processed batch 18/40\n",
            "    Processed batch 19/40\n",
            "    Processed batch 20/40\n",
            "    Processed batch 21/40\n",
            "    Processed batch 22/40\n",
            "    Processed batch 23/40\n",
            "    Processed batch 24/40\n",
            "    Processed batch 25/40\n",
            "    Processed batch 26/40\n",
            "    Processed batch 27/40\n",
            "    Processed batch 28/40\n",
            "    Processed batch 29/40\n",
            "    Processed batch 30/40\n",
            "    Processed batch 31/40\n",
            "    Processed batch 32/40\n",
            "    Processed batch 33/40\n",
            "    Processed batch 34/40\n",
            "    Processed batch 35/40\n",
            "    Processed batch 36/40\n",
            "    Processed batch 37/40\n",
            "    Processed batch 38/40\n",
            "    Processed batch 39/40\n",
            "    Processed batch 40/40\n",
            "Epoch 6/10\n",
            "    Processed batch 1/40\n",
            "    Processed batch 2/40\n",
            "    Processed batch 3/40\n",
            "    Processed batch 4/40\n",
            "    Processed batch 5/40\n",
            "    Processed batch 6/40\n",
            "    Processed batch 7/40\n",
            "    Processed batch 8/40\n",
            "    Processed batch 9/40\n",
            "    Processed batch 10/40\n",
            "    Processed batch 11/40\n",
            "    Processed batch 12/40\n",
            "    Processed batch 13/40\n",
            "    Processed batch 14/40\n",
            "    Processed batch 15/40\n",
            "    Processed batch 16/40\n",
            "    Processed batch 17/40\n",
            "    Processed batch 18/40\n",
            "    Processed batch 19/40\n",
            "    Processed batch 20/40\n",
            "    Processed batch 21/40\n",
            "    Processed batch 22/40\n",
            "    Processed batch 23/40\n",
            "    Processed batch 24/40\n",
            "    Processed batch 25/40\n",
            "    Processed batch 26/40\n",
            "    Processed batch 27/40\n",
            "    Processed batch 28/40\n",
            "    Processed batch 29/40\n",
            "    Processed batch 30/40\n",
            "    Processed batch 31/40\n",
            "    Processed batch 32/40\n",
            "    Processed batch 33/40\n",
            "    Processed batch 34/40\n",
            "    Processed batch 35/40\n",
            "    Processed batch 36/40\n",
            "    Processed batch 37/40\n",
            "    Processed batch 38/40\n",
            "    Processed batch 39/40\n",
            "    Processed batch 40/40\n",
            "Epoch 7/10\n",
            "    Processed batch 1/40\n",
            "    Processed batch 2/40\n",
            "    Processed batch 3/40\n",
            "    Processed batch 4/40\n",
            "    Processed batch 5/40\n",
            "    Processed batch 6/40\n",
            "    Processed batch 7/40\n",
            "    Processed batch 8/40\n",
            "    Processed batch 9/40\n",
            "    Processed batch 10/40\n",
            "    Processed batch 11/40\n",
            "    Processed batch 12/40\n",
            "    Processed batch 13/40\n",
            "    Processed batch 14/40\n",
            "    Processed batch 15/40\n",
            "    Processed batch 16/40\n",
            "    Processed batch 17/40\n",
            "    Processed batch 18/40\n",
            "    Processed batch 19/40\n",
            "    Processed batch 20/40\n",
            "    Processed batch 21/40\n",
            "    Processed batch 22/40\n",
            "    Processed batch 23/40\n",
            "    Processed batch 24/40\n",
            "    Processed batch 25/40\n",
            "    Processed batch 26/40\n",
            "    Processed batch 27/40\n",
            "    Processed batch 28/40\n",
            "    Processed batch 29/40\n",
            "    Processed batch 30/40\n",
            "    Processed batch 31/40\n",
            "    Processed batch 32/40\n",
            "    Processed batch 33/40\n",
            "    Processed batch 34/40\n",
            "    Processed batch 35/40\n",
            "    Processed batch 36/40\n",
            "    Processed batch 37/40\n",
            "    Processed batch 38/40\n",
            "    Processed batch 39/40\n",
            "    Processed batch 40/40\n",
            "Epoch 8/10\n",
            "    Processed batch 1/40\n",
            "    Processed batch 2/40\n",
            "    Processed batch 3/40\n",
            "    Processed batch 4/40\n",
            "    Processed batch 5/40\n",
            "    Processed batch 6/40\n",
            "    Processed batch 7/40\n",
            "    Processed batch 8/40\n",
            "    Processed batch 9/40\n",
            "    Processed batch 10/40\n",
            "    Processed batch 11/40\n",
            "    Processed batch 12/40\n",
            "    Processed batch 13/40\n",
            "    Processed batch 14/40\n",
            "    Processed batch 15/40\n",
            "    Processed batch 16/40\n",
            "    Processed batch 17/40\n",
            "    Processed batch 18/40\n",
            "    Processed batch 19/40\n",
            "    Processed batch 20/40\n",
            "    Processed batch 21/40\n",
            "    Processed batch 22/40\n",
            "    Processed batch 23/40\n",
            "    Processed batch 24/40\n",
            "    Processed batch 25/40\n",
            "    Processed batch 26/40\n",
            "    Processed batch 27/40\n",
            "    Processed batch 28/40\n",
            "    Processed batch 29/40\n",
            "    Processed batch 30/40\n",
            "    Processed batch 31/40\n",
            "    Processed batch 32/40\n",
            "    Processed batch 33/40\n",
            "    Processed batch 34/40\n",
            "    Processed batch 35/40\n",
            "    Processed batch 36/40\n",
            "    Processed batch 37/40\n",
            "    Processed batch 38/40\n",
            "    Processed batch 39/40\n",
            "    Processed batch 40/40\n",
            "Epoch 9/10\n",
            "    Processed batch 1/40\n",
            "    Processed batch 2/40\n",
            "    Processed batch 3/40\n",
            "    Processed batch 4/40\n",
            "    Processed batch 5/40\n",
            "    Processed batch 6/40\n",
            "    Processed batch 7/40\n",
            "    Processed batch 8/40\n",
            "    Processed batch 9/40\n",
            "    Processed batch 10/40\n",
            "    Processed batch 11/40\n",
            "    Processed batch 12/40\n",
            "    Processed batch 13/40\n",
            "    Processed batch 14/40\n",
            "    Processed batch 15/40\n",
            "    Processed batch 16/40\n",
            "    Processed batch 17/40\n",
            "    Processed batch 18/40\n",
            "    Processed batch 19/40\n",
            "    Processed batch 20/40\n",
            "    Processed batch 21/40\n",
            "    Processed batch 22/40\n",
            "    Processed batch 23/40\n",
            "    Processed batch 24/40\n",
            "    Processed batch 25/40\n",
            "    Processed batch 26/40\n",
            "    Processed batch 27/40\n",
            "    Processed batch 28/40\n",
            "    Processed batch 29/40\n",
            "    Processed batch 30/40\n",
            "    Processed batch 31/40\n",
            "    Processed batch 32/40\n",
            "    Processed batch 33/40\n",
            "    Processed batch 34/40\n",
            "    Processed batch 35/40\n",
            "    Processed batch 36/40\n",
            "    Processed batch 37/40\n",
            "    Processed batch 38/40\n",
            "    Processed batch 39/40\n",
            "    Processed batch 40/40\n",
            "Epoch 10/10\n",
            "    Processed batch 1/40\n",
            "    Processed batch 2/40\n",
            "    Processed batch 3/40\n",
            "    Processed batch 4/40\n",
            "    Processed batch 5/40\n",
            "    Processed batch 6/40\n",
            "    Processed batch 7/40\n",
            "    Processed batch 8/40\n",
            "    Processed batch 9/40\n",
            "    Processed batch 10/40\n",
            "    Processed batch 11/40\n",
            "    Processed batch 12/40\n",
            "    Processed batch 13/40\n",
            "    Processed batch 14/40\n",
            "    Processed batch 15/40\n",
            "    Processed batch 16/40\n",
            "    Processed batch 17/40\n",
            "    Processed batch 18/40\n",
            "    Processed batch 19/40\n",
            "    Processed batch 20/40\n",
            "    Processed batch 21/40\n",
            "    Processed batch 22/40\n",
            "    Processed batch 23/40\n",
            "    Processed batch 24/40\n",
            "    Processed batch 25/40\n",
            "    Processed batch 26/40\n",
            "    Processed batch 27/40\n",
            "    Processed batch 28/40\n",
            "    Processed batch 29/40\n",
            "    Processed batch 30/40\n",
            "    Processed batch 31/40\n",
            "    Processed batch 32/40\n",
            "    Processed batch 33/40\n",
            "    Processed batch 34/40\n",
            "    Processed batch 35/40\n",
            "    Processed batch 36/40\n",
            "    Processed batch 37/40\n",
            "    Processed batch 38/40\n",
            "    Processed batch 39/40\n",
            "    Processed batch 40/40\n",
            "Layer 1 training complete.\n",
            "Transformed data to hidden representation for next layer.\n",
            "\n",
            "\n",
            "Initializing Layer 2 with 64 hidden units...\n",
            "Epoch 1/10\n",
            "    Processed batch 1/40\n",
            "    Processed batch 2/40\n",
            "    Processed batch 3/40\n",
            "    Processed batch 4/40\n",
            "    Processed batch 5/40\n",
            "    Processed batch 6/40\n",
            "    Processed batch 7/40\n",
            "    Processed batch 8/40\n",
            "    Processed batch 9/40\n",
            "    Processed batch 10/40\n",
            "    Processed batch 11/40\n",
            "    Processed batch 12/40\n",
            "    Processed batch 13/40\n",
            "    Processed batch 14/40\n",
            "    Processed batch 15/40\n",
            "    Processed batch 16/40\n",
            "    Processed batch 17/40\n",
            "    Processed batch 18/40\n",
            "    Processed batch 19/40\n",
            "    Processed batch 20/40\n",
            "    Processed batch 21/40\n",
            "    Processed batch 22/40\n",
            "    Processed batch 23/40\n",
            "    Processed batch 24/40\n",
            "    Processed batch 25/40\n",
            "    Processed batch 26/40\n",
            "    Processed batch 27/40\n",
            "    Processed batch 28/40\n",
            "    Processed batch 29/40\n",
            "    Processed batch 30/40\n",
            "    Processed batch 31/40\n",
            "    Processed batch 32/40\n",
            "    Processed batch 33/40\n",
            "    Processed batch 34/40\n",
            "    Processed batch 35/40\n",
            "    Processed batch 36/40\n",
            "    Processed batch 37/40\n",
            "    Processed batch 38/40\n",
            "    Processed batch 39/40\n",
            "    Processed batch 40/40\n",
            "Epoch 2/10\n",
            "    Processed batch 1/40\n",
            "    Processed batch 2/40\n",
            "    Processed batch 3/40\n",
            "    Processed batch 4/40\n",
            "    Processed batch 5/40\n",
            "    Processed batch 6/40\n",
            "    Processed batch 7/40\n",
            "    Processed batch 8/40\n",
            "    Processed batch 9/40\n",
            "    Processed batch 10/40\n",
            "    Processed batch 11/40\n",
            "    Processed batch 12/40\n",
            "    Processed batch 13/40\n",
            "    Processed batch 14/40\n",
            "    Processed batch 15/40\n",
            "    Processed batch 16/40\n",
            "    Processed batch 17/40\n",
            "    Processed batch 18/40\n",
            "    Processed batch 19/40\n",
            "    Processed batch 20/40\n",
            "    Processed batch 21/40\n",
            "    Processed batch 22/40\n",
            "    Processed batch 23/40\n",
            "    Processed batch 24/40\n",
            "    Processed batch 25/40\n",
            "    Processed batch 26/40\n",
            "    Processed batch 27/40\n",
            "    Processed batch 28/40\n",
            "    Processed batch 29/40\n",
            "    Processed batch 30/40\n",
            "    Processed batch 31/40\n",
            "    Processed batch 32/40\n",
            "    Processed batch 33/40\n",
            "    Processed batch 34/40\n",
            "    Processed batch 35/40\n",
            "    Processed batch 36/40\n",
            "    Processed batch 37/40\n",
            "    Processed batch 38/40\n",
            "    Processed batch 39/40\n",
            "    Processed batch 40/40\n",
            "Epoch 3/10\n",
            "    Processed batch 1/40\n",
            "    Processed batch 2/40\n",
            "    Processed batch 3/40\n",
            "    Processed batch 4/40\n",
            "    Processed batch 5/40\n",
            "    Processed batch 6/40\n",
            "    Processed batch 7/40\n",
            "    Processed batch 8/40\n",
            "    Processed batch 9/40\n",
            "    Processed batch 10/40\n",
            "    Processed batch 11/40\n",
            "    Processed batch 12/40\n",
            "    Processed batch 13/40\n",
            "    Processed batch 14/40\n",
            "    Processed batch 15/40\n",
            "    Processed batch 16/40\n",
            "    Processed batch 17/40\n",
            "    Processed batch 18/40\n",
            "    Processed batch 19/40\n",
            "    Processed batch 20/40\n",
            "    Processed batch 21/40\n",
            "    Processed batch 22/40\n",
            "    Processed batch 23/40\n",
            "    Processed batch 24/40\n",
            "    Processed batch 25/40\n",
            "    Processed batch 26/40\n",
            "    Processed batch 27/40\n",
            "    Processed batch 28/40\n",
            "    Processed batch 29/40\n",
            "    Processed batch 30/40\n",
            "    Processed batch 31/40\n",
            "    Processed batch 32/40\n",
            "    Processed batch 33/40\n",
            "    Processed batch 34/40\n",
            "    Processed batch 35/40\n",
            "    Processed batch 36/40\n",
            "    Processed batch 37/40\n",
            "    Processed batch 38/40\n",
            "    Processed batch 39/40\n",
            "    Processed batch 40/40\n",
            "Epoch 4/10\n",
            "    Processed batch 1/40\n",
            "    Processed batch 2/40\n",
            "    Processed batch 3/40\n",
            "    Processed batch 4/40\n",
            "    Processed batch 5/40\n",
            "    Processed batch 6/40\n",
            "    Processed batch 7/40\n",
            "    Processed batch 8/40\n",
            "    Processed batch 9/40\n",
            "    Processed batch 10/40\n",
            "    Processed batch 11/40\n",
            "    Processed batch 12/40\n",
            "    Processed batch 13/40\n",
            "    Processed batch 14/40\n",
            "    Processed batch 15/40\n",
            "    Processed batch 16/40\n",
            "    Processed batch 17/40\n",
            "    Processed batch 18/40\n",
            "    Processed batch 19/40\n",
            "    Processed batch 20/40\n",
            "    Processed batch 21/40\n",
            "    Processed batch 22/40\n",
            "    Processed batch 23/40\n",
            "    Processed batch 24/40\n",
            "    Processed batch 25/40\n",
            "    Processed batch 26/40\n",
            "    Processed batch 27/40\n",
            "    Processed batch 28/40\n",
            "    Processed batch 29/40\n",
            "    Processed batch 30/40\n",
            "    Processed batch 31/40\n",
            "    Processed batch 32/40\n",
            "    Processed batch 33/40\n",
            "    Processed batch 34/40\n",
            "    Processed batch 35/40\n",
            "    Processed batch 36/40\n",
            "    Processed batch 37/40\n",
            "    Processed batch 38/40\n",
            "    Processed batch 39/40\n",
            "    Processed batch 40/40\n",
            "Epoch 5/10\n",
            "    Processed batch 1/40\n",
            "    Processed batch 2/40\n",
            "    Processed batch 3/40\n",
            "    Processed batch 4/40\n",
            "    Processed batch 5/40\n",
            "    Processed batch 6/40\n",
            "    Processed batch 7/40\n",
            "    Processed batch 8/40\n",
            "    Processed batch 9/40\n",
            "    Processed batch 10/40\n",
            "    Processed batch 11/40\n",
            "    Processed batch 12/40\n",
            "    Processed batch 13/40\n",
            "    Processed batch 14/40\n",
            "    Processed batch 15/40\n",
            "    Processed batch 16/40\n",
            "    Processed batch 17/40\n",
            "    Processed batch 18/40\n",
            "    Processed batch 19/40\n",
            "    Processed batch 20/40\n",
            "    Processed batch 21/40\n",
            "    Processed batch 22/40\n",
            "    Processed batch 23/40\n",
            "    Processed batch 24/40\n",
            "    Processed batch 25/40\n",
            "    Processed batch 26/40\n",
            "    Processed batch 27/40\n",
            "    Processed batch 28/40\n",
            "    Processed batch 29/40\n",
            "    Processed batch 30/40\n",
            "    Processed batch 31/40\n",
            "    Processed batch 32/40\n",
            "    Processed batch 33/40\n",
            "    Processed batch 34/40\n",
            "    Processed batch 35/40\n",
            "    Processed batch 36/40\n",
            "    Processed batch 37/40\n",
            "    Processed batch 38/40\n",
            "    Processed batch 39/40\n",
            "    Processed batch 40/40\n",
            "Epoch 6/10\n",
            "    Processed batch 1/40\n",
            "    Processed batch 2/40\n",
            "    Processed batch 3/40\n",
            "    Processed batch 4/40\n",
            "    Processed batch 5/40\n",
            "    Processed batch 6/40\n",
            "    Processed batch 7/40\n",
            "    Processed batch 8/40\n",
            "    Processed batch 9/40\n",
            "    Processed batch 10/40\n",
            "    Processed batch 11/40\n",
            "    Processed batch 12/40\n",
            "    Processed batch 13/40\n",
            "    Processed batch 14/40\n",
            "    Processed batch 15/40\n",
            "    Processed batch 16/40\n",
            "    Processed batch 17/40\n",
            "    Processed batch 18/40\n",
            "    Processed batch 19/40\n",
            "    Processed batch 20/40\n",
            "    Processed batch 21/40\n",
            "    Processed batch 22/40\n",
            "    Processed batch 23/40\n",
            "    Processed batch 24/40\n",
            "    Processed batch 25/40\n",
            "    Processed batch 26/40\n",
            "    Processed batch 27/40\n",
            "    Processed batch 28/40\n",
            "    Processed batch 29/40\n",
            "    Processed batch 30/40\n",
            "    Processed batch 31/40\n",
            "    Processed batch 32/40\n",
            "    Processed batch 33/40\n",
            "    Processed batch 34/40\n",
            "    Processed batch 35/40\n",
            "    Processed batch 36/40\n",
            "    Processed batch 37/40\n",
            "    Processed batch 38/40\n",
            "    Processed batch 39/40\n",
            "    Processed batch 40/40\n",
            "Epoch 7/10\n",
            "    Processed batch 1/40\n",
            "    Processed batch 2/40\n",
            "    Processed batch 3/40\n",
            "    Processed batch 4/40\n",
            "    Processed batch 5/40\n",
            "    Processed batch 6/40\n",
            "    Processed batch 7/40\n",
            "    Processed batch 8/40\n",
            "    Processed batch 9/40\n",
            "    Processed batch 10/40\n",
            "    Processed batch 11/40\n",
            "    Processed batch 12/40\n",
            "    Processed batch 13/40\n",
            "    Processed batch 14/40\n",
            "    Processed batch 15/40\n",
            "    Processed batch 16/40\n",
            "    Processed batch 17/40\n",
            "    Processed batch 18/40\n",
            "    Processed batch 19/40\n",
            "    Processed batch 20/40\n",
            "    Processed batch 21/40\n",
            "    Processed batch 22/40\n",
            "    Processed batch 23/40\n",
            "    Processed batch 24/40\n",
            "    Processed batch 25/40\n",
            "    Processed batch 26/40\n",
            "    Processed batch 27/40\n",
            "    Processed batch 28/40\n",
            "    Processed batch 29/40\n",
            "    Processed batch 30/40\n",
            "    Processed batch 31/40\n",
            "    Processed batch 32/40\n",
            "    Processed batch 33/40\n",
            "    Processed batch 34/40\n",
            "    Processed batch 35/40\n",
            "    Processed batch 36/40\n",
            "    Processed batch 37/40\n",
            "    Processed batch 38/40\n",
            "    Processed batch 39/40\n",
            "    Processed batch 40/40\n",
            "Epoch 8/10\n",
            "    Processed batch 1/40\n",
            "    Processed batch 2/40\n",
            "    Processed batch 3/40\n",
            "    Processed batch 4/40\n",
            "    Processed batch 5/40\n",
            "    Processed batch 6/40\n",
            "    Processed batch 7/40\n",
            "    Processed batch 8/40\n",
            "    Processed batch 9/40\n",
            "    Processed batch 10/40\n",
            "    Processed batch 11/40\n",
            "    Processed batch 12/40\n",
            "    Processed batch 13/40\n",
            "    Processed batch 14/40\n",
            "    Processed batch 15/40\n",
            "    Processed batch 16/40\n",
            "    Processed batch 17/40\n",
            "    Processed batch 18/40\n",
            "    Processed batch 19/40\n",
            "    Processed batch 20/40\n",
            "    Processed batch 21/40\n",
            "    Processed batch 22/40\n",
            "    Processed batch 23/40\n",
            "    Processed batch 24/40\n",
            "    Processed batch 25/40\n",
            "    Processed batch 26/40\n",
            "    Processed batch 27/40\n",
            "    Processed batch 28/40\n",
            "    Processed batch 29/40\n",
            "    Processed batch 30/40\n",
            "    Processed batch 31/40\n",
            "    Processed batch 32/40\n",
            "    Processed batch 33/40\n",
            "    Processed batch 34/40\n",
            "    Processed batch 35/40\n",
            "    Processed batch 36/40\n",
            "    Processed batch 37/40\n",
            "    Processed batch 38/40\n",
            "    Processed batch 39/40\n",
            "    Processed batch 40/40\n",
            "Epoch 9/10\n",
            "    Processed batch 1/40\n",
            "    Processed batch 2/40\n",
            "    Processed batch 3/40\n",
            "    Processed batch 4/40\n",
            "    Processed batch 5/40\n",
            "    Processed batch 6/40\n",
            "    Processed batch 7/40\n",
            "    Processed batch 8/40\n",
            "    Processed batch 9/40\n",
            "    Processed batch 10/40\n",
            "    Processed batch 11/40\n",
            "    Processed batch 12/40\n",
            "    Processed batch 13/40\n",
            "    Processed batch 14/40\n",
            "    Processed batch 15/40\n",
            "    Processed batch 16/40\n",
            "    Processed batch 17/40\n",
            "    Processed batch 18/40\n",
            "    Processed batch 19/40\n",
            "    Processed batch 20/40\n",
            "    Processed batch 21/40\n",
            "    Processed batch 22/40\n",
            "    Processed batch 23/40\n",
            "    Processed batch 24/40\n",
            "    Processed batch 25/40\n",
            "    Processed batch 26/40\n",
            "    Processed batch 27/40\n",
            "    Processed batch 28/40\n",
            "    Processed batch 29/40\n",
            "    Processed batch 30/40\n",
            "    Processed batch 31/40\n",
            "    Processed batch 32/40\n",
            "    Processed batch 33/40\n",
            "    Processed batch 34/40\n",
            "    Processed batch 35/40\n",
            "    Processed batch 36/40\n",
            "    Processed batch 37/40\n",
            "    Processed batch 38/40\n",
            "    Processed batch 39/40\n",
            "    Processed batch 40/40\n",
            "Epoch 10/10\n",
            "    Processed batch 1/40\n",
            "    Processed batch 2/40\n",
            "    Processed batch 3/40\n",
            "    Processed batch 4/40\n",
            "    Processed batch 5/40\n",
            "    Processed batch 6/40\n",
            "    Processed batch 7/40\n",
            "    Processed batch 8/40\n",
            "    Processed batch 9/40\n",
            "    Processed batch 10/40\n",
            "    Processed batch 11/40\n",
            "    Processed batch 12/40\n",
            "    Processed batch 13/40\n",
            "    Processed batch 14/40\n",
            "    Processed batch 15/40\n",
            "    Processed batch 16/40\n",
            "    Processed batch 17/40\n",
            "    Processed batch 18/40\n",
            "    Processed batch 19/40\n",
            "    Processed batch 20/40\n",
            "    Processed batch 21/40\n",
            "    Processed batch 22/40\n",
            "    Processed batch 23/40\n",
            "    Processed batch 24/40\n",
            "    Processed batch 25/40\n",
            "    Processed batch 26/40\n",
            "    Processed batch 27/40\n",
            "    Processed batch 28/40\n",
            "    Processed batch 29/40\n",
            "    Processed batch 30/40\n",
            "    Processed batch 31/40\n",
            "    Processed batch 32/40\n",
            "    Processed batch 33/40\n",
            "    Processed batch 34/40\n",
            "    Processed batch 35/40\n",
            "    Processed batch 36/40\n",
            "    Processed batch 37/40\n",
            "    Processed batch 38/40\n",
            "    Processed batch 39/40\n",
            "    Processed batch 40/40\n",
            "Layer 2 training complete.\n",
            "Transformed data to hidden representation for next layer.\n",
            "\n",
            "RBM Pretraining Complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform data using all CRBM layers\n",
        "rbm_input = X_train\n",
        "for rbm in rbm_layers:\n",
        "    _, rbm_input = rbm.sample_h(rbm_input)\n",
        "\n",
        "# Final layer transformation\n",
        "_, rbm_features = rbm_layers[-1].sample_h(rbm_input)\n",
        "rbm_features = np.expand_dims(rbm_features, axis=-1)  # Add an extra dimension for CNN input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQh7zJUqwSWh",
        "outputId": "8d0a0791-5a71-4db5-b530-ac191462788a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-cd4541fada9f>:30: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define CNN Model\n",
        "model = Sequential([\n",
        "    Conv1D(64, kernel_size=3, activation='relu', input_shape=(rbm_features.shape[1], 1), kernel_regularizer=l2(0.01)),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Conv1D(128, kernel_size=3, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "    Dropout(0.5),\n",
        "    Dense(len(label_encoder.classes_), activation='softmax')  # Output layer with softmax for classification\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdA47Ob9wUGw",
        "outputId": "9e575b34-f8d6-47ef-ed8f-08aa42697d4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "kl2-rabeyJon",
        "outputId": "3943e35e-4429-40aa-c712-a8447c65a0f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m64\u001b[0m)              │             \u001b[38;5;34m256\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m64\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │          \u001b[38;5;34m24,704\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1792\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │         \u001b[38;5;34m459,008\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)                   │           \u001b[38;5;34m1,285\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1792</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">459,008</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,285</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m485,253\u001b[0m (1.85 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">485,253</span> (1.85 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m485,253\u001b[0m (1.85 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">485,253</span> (1.85 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "By_SuxE8wVeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Early stopping to avoid overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "IAuSR8-7wWlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model with class weights\n",
        "history = model.fit(rbm_features, y_train, epochs=50, batch_size=32, validation_split=0.2,\n",
        "                    callbacks=[early_stopping], verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubphKvQcwXr7",
        "outputId": "13fd00d5-12c8-4d05-8a03-df69e9020617"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 26ms/step - accuracy: 0.1804 - loss: 4.7147 - val_accuracy: 0.2071 - val_loss: 1.8816\n",
            "Epoch 2/50\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.1966 - loss: 1.7810 - val_accuracy: 0.2091 - val_loss: 1.6396\n",
            "Epoch 3/50\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.2139 - loss: 1.6302 - val_accuracy: 0.2091 - val_loss: 1.6152\n",
            "Epoch 4/50\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.1776 - loss: 1.6144 - val_accuracy: 0.2091 - val_loss: 1.6114\n",
            "Epoch 5/50\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.2147 - loss: 1.6111 - val_accuracy: 0.2091 - val_loss: 1.6104\n",
            "Epoch 6/50\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.2094 - loss: 1.6104 - val_accuracy: 0.2091 - val_loss: 1.6099\n",
            "Epoch 7/50\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.2020 - loss: 1.6104 - val_accuracy: 0.2091 - val_loss: 1.6099\n",
            "Epoch 8/50\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.2005 - loss: 1.6097 - val_accuracy: 0.2091 - val_loss: 1.6096\n",
            "Epoch 9/50\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.2231 - loss: 1.6093 - val_accuracy: 0.2091 - val_loss: 1.6095\n",
            "Epoch 10/50\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.2002 - loss: 1.6100 - val_accuracy: 0.2091 - val_loss: 1.6093\n",
            "Epoch 11/50\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 0.1964 - loss: 1.6096 - val_accuracy: 0.2091 - val_loss: 1.6094\n",
            "Epoch 12/50\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.1969 - loss: 1.6099 - val_accuracy: 0.2091 - val_loss: 1.6094\n",
            "Epoch 13/50\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.1921 - loss: 1.6099 - val_accuracy: 0.2091 - val_loss: 1.6094\n",
            "Epoch 14/50\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.1895 - loss: 1.6096 - val_accuracy: 0.2091 - val_loss: 1.6093\n",
            "Epoch 15/50\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.2125 - loss: 1.6090 - val_accuracy: 0.2091 - val_loss: 1.6093\n",
            "Epoch 16/50\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.2030 - loss: 1.6098 - val_accuracy: 0.2091 - val_loss: 1.6094\n",
            "Epoch 17/50\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.1975 - loss: 1.6097 - val_accuracy: 0.2091 - val_loss: 1.6095\n",
            "Epoch 18/50\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.1825 - loss: 1.6099 - val_accuracy: 0.2091 - val_loss: 1.6093\n",
            "Epoch 19/50\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.2007 - loss: 1.6094 - val_accuracy: 0.2091 - val_loss: 1.6093\n",
            "Epoch 20/50\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.2066 - loss: 1.6094 - val_accuracy: 0.2091 - val_loss: 1.6092\n",
            "Epoch 21/50\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.1887 - loss: 1.6097 - val_accuracy: 0.2091 - val_loss: 1.6093\n",
            "Epoch 22/50\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.1885 - loss: 1.6099 - val_accuracy: 0.2091 - val_loss: 1.6094\n",
            "Epoch 23/50\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.2057 - loss: 1.6097 - val_accuracy: 0.2091 - val_loss: 1.6095\n",
            "Epoch 24/50\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.2005 - loss: 1.6096 - val_accuracy: 0.2091 - val_loss: 1.6094\n",
            "Epoch 25/50\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.2060 - loss: 1.6094 - val_accuracy: 0.2091 - val_loss: 1.6094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another method"
      ],
      "metadata": {
        "id": "XbV6jzzx0un2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save RBM features in csv"
      ],
      "metadata": {
        "id": "NOSqv2vU0mXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# CRBM Class Definition (for unsupervised pretraining)\n",
        "class CRBM:\n",
        "    def __init__(self, n_visible, n_hidden, learning_rate=0.01):\n",
        "        self.n_visible = n_visible\n",
        "        self.n_hidden = n_hidden\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weights = np.random.normal(0, 0.1, (n_visible, n_hidden))\n",
        "        self.h_bias = np.zeros(n_hidden)\n",
        "        self.v_bias = np.zeros(n_visible)\n",
        "\n",
        "    def sample_h(self, v):\n",
        "        h_prob = self._sigmoid(np.dot(v, self.weights) + self.h_bias)\n",
        "        return h_prob, np.random.binomial(1, h_prob)\n",
        "\n",
        "    def sample_v(self, h):\n",
        "        v_prob = self._sigmoid(np.dot(h, self.weights.T) + self.v_bias)\n",
        "        return v_prob, np.random.binomial(1, v_prob)\n",
        "\n",
        "    def train(self, data, epochs=100):\n",
        "        for epoch in range(epochs):\n",
        "            v0 = data\n",
        "            h0_prob, h0 = self.sample_h(v0)\n",
        "            v1_prob, v1 = self.sample_v(h0)\n",
        "            h1_prob, h1 = self.sample_h(v1)\n",
        "            self.weights += np.dot(v0.T, h0 - h1) * self.learning_rate\n",
        "            self.v_bias += np.mean(v0 - v1, axis=0) * self.learning_rate\n",
        "            self.h_bias += np.mean(h0 - h1, axis=0) * self.learning_rate\n",
        "\n",
        "    def _sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Load raw audio files and extract features\n",
        "data_dir = '/content/drive/MyDrive/New_indexes_mp3'  # Path to the directory containing audio files\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "# Loop through all files in the directory\n",
        "for filename in os.listdir(data_dir):\n",
        "    if filename.endswith('.mp3'):\n",
        "        # Extract genre from the filename\n",
        "        genre = filename.split('_')[0]  # Assuming format is 'genre_index.mp3'\n",
        "        file_path = os.path.join(data_dir, filename)\n",
        "\n",
        "        # Load raw audio signal\n",
        "        signal, sr = librosa.load(file_path, sr=22050)\n",
        "        signal = signal[:len(signal) // 2]  # Optionally truncate to a fixed size\n",
        "\n",
        "        # Flatten signal to 1D vector (you can also extract other features)\n",
        "        feature = signal.flatten()\n",
        "\n",
        "        # Append the features and labels\n",
        "        features.append(feature)\n",
        "        labels.append(genre)\n",
        "\n",
        "# Convert features to numpy array\n",
        "X = np.array(features)\n",
        "\n",
        "# Label Encoding\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(labels)  # Convert labels to integers\n",
        "y_onehot = to_categorical(y_encoded)  # One-hot encode labels\n",
        "\n",
        "# Normalize features\n",
        "X = (X - np.mean(X)) / np.std(X)  # Standard normalization\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n",
        "\n",
        "# CRBM Pretraining\n",
        "num_layers = 2\n",
        "hidden_units = 64\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "rbm_input = X_train\n",
        "rbm_layers = []\n",
        "\n",
        "# Train the CRBM layers\n",
        "for layer in range(num_layers):\n",
        "    rbm = CRBM(n_visible=rbm_input.shape[1], n_hidden=hidden_units)\n",
        "    print(f\"\\nInitializing Layer {layer + 1} with {hidden_units} hidden units...\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "        for i in range(0, rbm_input.shape[0], batch_size):\n",
        "            batch = rbm_input[i:i + batch_size]\n",
        "            rbm.train(batch, epochs=1)  # Train for 1 epoch per batch\n",
        "            print(f\"    Processed batch {i//batch_size + 1}/{rbm_input.shape[0]//batch_size + 1}\")\n",
        "\n",
        "    rbm_layers.append(rbm)\n",
        "    print(f\"Layer {layer + 1} training complete.\")\n",
        "    _, rbm_input = rbm.sample_h(rbm_input)  # Transform data to hidden representation for next layer\n",
        "    print(f\"Transformed data to hidden representation for next layer.\\n\")\n",
        "\n",
        "print(\"RBM Pretraining Complete!\")\n",
        "\n",
        "# Transform data using all CRBM layers\n",
        "rbm_input = X_train\n",
        "for rbm in rbm_layers:\n",
        "    _, rbm_input = rbm.sample_h(rbm_input)\n",
        "\n",
        "# Final layer transformation\n",
        "_, rbm_features = rbm_layers[-1].sample_h(rbm_input)\n",
        "\n",
        "# Save RBM features and labels to CSV\n",
        "data_with_labels = np.hstack((rbm_features, y_train))  # Add the labels as the last column\n",
        "csv_file_path = '/content/drive/MyDrive/crbm_features_with_labels.csv'\n",
        "pd.DataFrame(data_with_labels).to_csv(csv_file_path, index=False, header=False)\n",
        "\n",
        "print(f\"Data saved to {csv_file_path}\")\n"
      ],
      "metadata": {
        "id": "qLiYMPxCF_pE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/crbm_features_with_labels.csv')"
      ],
      "metadata": {
        "id": "c2CiIJ5UHi89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fig_tU5hHmuL",
        "outputId": "d2975fdf-11c8-4299-83e8-31354bccfd2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2530, 69)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Step 1: Load the CSV file\n",
        "csv_file = '/content/crbm_features_with_labels.csv'  # Replace with the actual file path\n",
        "data = pd.read_csv(csv_file)\n",
        "\n",
        "# Step 2: Separate features and labels\n",
        "X = data.iloc[:, :-1].values  # All columns except the last (features)\n",
        "y = data.iloc[:, -1].values   # Last column (label)\n",
        "\n",
        "# Step 3: Label Encoding and One-hot Encoding\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)  # Encode labels as integers\n",
        "y_onehot = to_categorical(y_encoded)  # One-hot encode labels\n",
        "\n",
        "# Step 4: Normalize features\n",
        "X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)  # Standard normalization\n",
        "\n",
        "# Step 5: Reshape features for CNN (1D convolution expects 3D input: samples, timesteps, features)\n",
        "X = np.expand_dims(X, axis=-1)  # Add an extra dimension for CNN input\n",
        "\n",
        "# Step 6: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 7: Define the CNN model\n",
        "model = Sequential([\n",
        "    Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Conv1D(128, kernel_size=3, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(len(label_encoder.classes_), activation='softmax')  # Output layer\n",
        "])\n",
        "\n",
        "# Step 8: Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 9: Early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Step 10: Train the model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test),\n",
        "                    callbacks=[early_stopping], verbose=1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H86kb0-5Hnq4",
        "outputId": "a17769ce-4502-4d6f-af52-243c51a0b595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 55ms/step - accuracy: 0.7754 - loss: 0.5270 - val_accuracy: 0.8043 - val_loss: 0.4279\n",
            "Epoch 2/50\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7954 - loss: 0.4488 - val_accuracy: 0.8063 - val_loss: 0.4153\n",
            "Epoch 3/50\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7855 - loss: 0.4219 - val_accuracy: 0.8063 - val_loss: 0.4131\n",
            "Epoch 4/50\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7978 - loss: 0.3989 - val_accuracy: 0.8043 - val_loss: 0.4187\n",
            "Epoch 5/50\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8023 - loss: 0.3927 - val_accuracy: 0.8103 - val_loss: 0.4210\n",
            "Epoch 6/50\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8132 - loss: 0.3633 - val_accuracy: 0.7866 - val_loss: 0.4326\n",
            "Epoch 7/50\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8223 - loss: 0.3512 - val_accuracy: 0.8024 - val_loss: 0.4422\n",
            "Epoch 8/50\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8216 - loss: 0.3431 - val_accuracy: 0.8083 - val_loss: 0.4394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model as well\n",
        "model.save('/content/drive/MyDrive/CRBM_store_in_CSV.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8bzX_B7IVh9",
        "outputId": "0d7f569a-b86a-4fbf-81ea-5a9414077e9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hr-i6_euIAqA",
        "outputId": "2588bdbc-aeaa-4d0a-cd4d-62af4b24176e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8110 - loss: 0.4082 \n",
            "Test Accuracy: 0.8063\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5DGizKGySo_-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}